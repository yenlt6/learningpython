{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated vs. Manual Testing\n",
    "Bình thường, ta hay thực hiện test mà không có kế hoạch. Bạn chỉ đang khám phá ứng dụng, phần mềm của bạn.\n",
    "\n",
    "Để có một bộ kiểm thử thủ công hoàn chỉnh, bạn cần lập danh sách tất cả các tính năng mà ứng dụng của bạn có, các loại đầu vào khác nhau mà ứng dụng có thể chấp nhận và kết quả mong đợi. Bây giờ, mỗi khi bạn thực hiện thay đổi đối với mã của mình, bạn cần phải xem qua từng mục trong danh sách đó và kiểm tra nó.\n",
    "\n",
    "Rõ ràng là không hay ho gì khi cứ lặp đi lặp lại các công việc như vậy. Đây là lúc cần kiểm thử tự động. Kiểm thử tự động là việc thực hiện kế hoạch kiểm thử của bạn (các phần của ứng dụng bạn muốn kiểm tra, thứ tự mà bạn muốn kiểm tra chúng và các phản hồi dự kiến) bằng tập lệnh thay vì con người ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests vs. Integration Tests\n",
    "Kiểm thử nhiều thành phần được gọi là **kiểm thử tích hợp**. Thách thức lớn với kiểm thử tích hợp là khi nó không cho kết quả mong đợi. Rất khó để chẩn đoán sự cố mà không thể xác định phần nào của hệ thống đang bị lỗi. Cho nên ta cần đến unittest. **Kiểm thử đơn vị** là bài kiểm tra nhỏ hơn, xem một thành phần duy nhất có hoạt động đúng cách hay không. Kiểm thử đơn vị giúp bạn tách biệt những gì bị hỏng trong ứng dụng của mình và khắc phục nó nhanh hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum([1, 2, 3]) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum([1, 1, 1]) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sum():\n",
    "    assert sum([1, 2, 3]) == 5, \"Should be 6\"\n",
    "\n",
    "def test_sum_tuple():\n",
    "    assert sum((1, 2, 3, 4)) == 9\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_sum()\n",
    "    test_sum_tuple()\n",
    "    print(\"Everything passed! Bạn vừa viết một test case đầu tiên!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Runner\n",
    "Viết test theo cách này chỉ hợp lý khi test code đơn giản, nhưng nếu nhiều hơn một testcase không đạt thì sao? Đây là nơi **test runners** xuất hiện. Test runner là một ứng dụng đặc biệt được thiết kế để chạy các test cases, kiểm tra kết quả đầu ra và cung cấp cho bạn các công cụ để gỡ lỗi và chẩn đoán các thử nghiệm và ứng dụng. Trong Python, ta có vài lựa chọn:\n",
    "* unittest\n",
    "* nose hoặc nose2 (đã từng phổ biến, nhưng đang dần bị thay thế bởi pytest)\n",
    "* pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unittest\n",
    "Hãy xem xét file main.py trong đề thi hackathon lần 1, unittest yêu cầu rằng:\n",
    "\n",
    "* Bạn đặt các tests vào các classes dưới dạng method\n",
    "* Bạn sử dụng một loạt các phương thức assert đặc biệt trong lớp unittest.TestCase thay vì câu lệnh assert được tích hợp sẵn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytest\n",
    "Hỗ trợ thực thi code unittest. pytest giúp việc viết tests đơn giản hơn với các hàm trong tệp Python bắt đầu bằng tên test_.\n",
    "\n",
    "pytest có một số tính năng tuyệt vời khác:\n",
    "\n",
    "* Hỗ trợ cho câu lệnh assert được tích hợp sẵn thay vì sử dụng các phương thức self.assert*() đặc biệt\n",
    "* Hỗ trợ lọc các test cases (python sample_skiptests.py hoặc pytest sample_skiptests.py)\n",
    "* Khả năng chạy lại từ lần kiểm thử thất bại gần nhất\n",
    "* Hệ sinh thái gồm hàng trăm plugin để mở rộng chức năng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample pytest\n",
    "File wallet.py chứa code mô phỏng hoạt động của ví điện tử và file test_wallet.py test các hoạt động của ví.\n",
    "\n",
    "Chạy tập con các test\n",
    "```\n",
    "cd testing/wallet\n",
    "\n",
    "# chạy những test case chứa từ spend trong function name\n",
    "pytest -k spend -v\n",
    "\n",
    "# chạy những test case được mark là ini (marker đã được đăng ký trong pytest.ini)\n",
    "pytest -m ini -v .\\test_wallet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cấu trúc của một Test Case\n",
    "Cấu trúc của một testcase nên tuân theo quy trình sau:\n",
    "* Tạo đầu vào của bạn\n",
    "* Thực thi mã đang được kiểm tra, thu nhận kết quả đầu ra\n",
    "* So sánh kết quả đầu ra với kết quả mong đợi\n",
    "\n",
    "Với việc phải test hàm sum() ở trên, bạn có thể cần kiểm tra nhiều hành vi khác nhau vd như???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Effect - Phản ứng phụ\n",
    "Khi bạn viết các test, việc xem xét giá trị trả về của một hàm thường không đơn giản như vậy. Thông thường, việc thực thi một đoạn mã sẽ thay đổi những thứ khác trong môi trường, chẳng hạn như thuộc tính của một lớp, một tệp trên hệ thống hoặc một giá trị trong cơ sở dữ liệu. Đây được gọi là tác dụng phụ và là một phần quan trọng của kiểm thử. Quyết định xem tác dụng phụ có cần được kiểm thử hay không trước khi đưa vào danh sách assertion của bạn.\n",
    "\n",
    "Nếu bạn thấy rằng đơn vị mã bạn muốn kiểm tra có nhiều tác dụng phụ, coder có thể đã vi phạm Nguyên tắc Trách nhiệm Đơn lẻ. Phá vỡ Nguyên tắc Trách nhiệm Đơn lẻ có nghĩa là đoạn mã đang làm quá nhiều thứ và tốt hơn hết là nên cấu trúc lại (refactor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Teardown\n",
    "`pytest -s sample_pytest.py`        # Thêm tham số -s để hiển thị stdout.\n",
    "## Fixtures\n",
    "Lưu ý các scope của fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tham số hóa các hàm test với @pytest.mark.parametrize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liệt kê các test chạy chậm nhất\n",
    "```\n",
    "pytest --durations=3\n",
    "=========================================== slowest 3 durations ============================================ \n",
    "9.76s call     tests/test_real_server.py::test_request_response\n",
    "4.32s call     test_login_user.py::test_login_no_password\n",
    "4.01s call     test_list_user.py::test_list_invaliduser\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giả lập (Mock) những đối tượng bên ngoài\n",
    "Kết nối với đối tác bên ngoài là bắt buộc đối với ứng dụng ngày nay. Tuy nhiên, bạn không sở hữu thư viện các bên ngoài đó, có nghĩa là bạn không thể kiểm soát các máy chủ lưu trữ nó, code bao gồm logic của nó hoặc dữ liệu được chuyển giữa nó và ứng dụng của bạn. Do đó bạn cần dùng Mock.\n",
    "\n",
    "Mock là một đối tượng giả mà bạn tạo ra để trông và hoạt động giống như dữ liệu thực. Bạn hoán đổi nó với đối tượng thực tế và đánh lừa hệ thống nghĩ rằng đối tượng mock đó là đối tượng thực sự.\n",
    "\n",
    "Đầu tiên, ta dùng một dịch vụ API online - JSON Placeholder cho mục đích testing trong bài này.\n",
    "1. Dùng [reqbin](https://reqbin.com/curl)\n",
    "\n",
    "`curl -X GET 'http://jsonplaceholder.typicode.com/todos'`\n",
    "\n",
    "2. TDD - Test Driven Development\n",
    "`pytest -k request --ignore=wallet --ignore=tests -v`\n",
    "\n",
    "3. Refactor code into files (services.py & constants.py)\n",
    "`pytest -m step3 --ignore=wallet --ignore=tests -v`\n",
    "4. First Mock (mock_get.return_value.ok = True)\n",
    "Test đã pass, vấn đề là dịch vụ của bạn vẫn đang truy cập trực tiếp vào máy chủ bên ngoài. Khi bạn gọi get_todos(), mã của bạn đang thực hiện yêu cầu tới API endpoint và trả về kết quả phụ thuộc vào máy chủ đó đang hoạt động. Ta sẽ tách logic khỏi thư viện thực tế bằng cách hoán đổi với 1 yêu cầu giả trả về cùng một dữ liệu.\n",
    "\n",
    "Khi chạy, test tìm mô-đun nơi thư viện request được khai báo, trong file services.py, và thay thế hàm request.get() bằng một đối tượng giả (mock). Đoạn code Test cũng yêu cầu mock hoạt động theo cách mà hàm dịch vụ thật hoạt động. Nếu bạn nhìn vào get_todos(), bạn sẽ thấy rằng sự thành công của hàm phụ thuộc vào `if response.ok: return True`. Đó là những gì dòng `mock_get.return_value.ok = True` đang làm. Khi thuộc tính ok được gọi trên mock, nó sẽ trả về True giống như đối tượng thực. Hàm get_todos() sẽ trả về một mock response và test sẽ pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Other ways to patch\n",
    "    1. `with patch('services.requests.get') as mock_get:`\n",
    "    2. Dùng patcher: `pytest -m step6 --ignore=wallet --ignore=tests -v`\n",
    "6. So sánh 3 cách:\n",
    "    * Sử dụng decorator khi tất cả code trong nội dung hàm test dùng chung 1 mock.\n",
    "    * Dùng context manager khi một đoạn code trong hàm test dùng 1 mock và đoạn code khác cần dùng chức năng thực.\n",
    "    * Dùng patcher khi bạn cần start và stop mocking chức năng một cách rõ ràng trong nhiều test (ví dụ: các chức năng setUp() và tearDown() trong một lớp test).\n",
    "\n",
    "    Để dùng @patch(): Bạn cung cấp một đường dẫn đến hàm bạn muốn mock. Nếu tìm thấy, patch() tạo một đối tượng Mock và hàm thực tạm thời được thay thế bằng mock. Khi test get_todos() được gọi, hàm sử dụng mock_get giống như cách nó sử dụng phương thức get() thực. Điều đó có nghĩa là nó gọi mock_get giống như một hàm và mong đợi nó trả về một đối tượng phản hồi.\n",
    "\n",
    "    Trong trường hợp này, đối tượng phản hồi là một đối tượng Response trong thư viện requests, đối tượng này có một số thuộc tính và phương thức. Bạn đã mock thuộc tính ok trong ví dụ trước. Đối tượng Response cũng có một hàm json() chuyển đổi nội dung chuỗi JSON thành một kiểu dữ liệu Python (ví dụ: một danh sách hoặc một dict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['assert_any_call', 'assert_called', 'assert_called_once', 'assert_called_once_with', 'assert_called_with', 'assert_has_calls', 'assert_not_called', 'attach_mock', 'call_args', 'call_args_list', 'call_count', 'called', 'configure_mock', 'method_calls', 'mock_add_spec', 'mock_calls', 'reset_mock', 'return_value', 'side_effect']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from unittest.mock import Mock, patch\n",
    "\n",
    "with patch('requests.get') as mock_get:\n",
    "    mock_get.return_value.ok = True\n",
    "    print(dir(mock_get.return_value.json))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Mock tất cả các hành vi của service: \n",
    "    * Ở bước 6 là mock thành công. Bây giờ cần mock cả dữ liệu\n",
    "8. Nếu ta có service khác gọi get_todos(), thì ta chỉ cần mock chính hàm get_todos() để test service mới mà không cần phải viết lại phần mock cho requests.get().\n",
    "9. Refactor code into Test classes.\n",
    "\n",
    "    * Chuyển các hàm test chung vào một lớp cho phép bạn dễ dàng test chúng cùng nhau như một nhóm. Bạn có thể yêu cầu pytest chạy một danh sách các hàm (như tôi đã làm), nhưng việc yêu cầu chạy 1 class sẽ dễ dàng hơn.\n",
    "    * Các test thường yêu cầu các bước tương tự để tạo và hủy dữ liệu được sử dụng bởi mỗi test. Các bước này có thể được gói gọn trong các hàm setup_class() và teardown_class() tương ứng để thực thi mã ở các giai đoạn thích hợp.\n",
    "    * Bạn có thể tạo các hàm tiện ích trên lớp để sử dụng lại logic được lặp lại giữa các hàm kiểm tra.\n",
    "\n",
    "```\n",
    "cd tests\n",
    "pytest -m step9 -v\n",
    "```\n",
    "10. Ta đang dựa vào giả định (khá nguy hiểm) là api data trả về của service thật không thay đổi. Để xác nhận dữ liệu nhận được khớp với dữ liệu test, ta dùng context manager để so sánh cấu trúc dữ liệu (key trong json) của 2 bên. --> Contract testing\n",
    "\n",
    "Như bạn có thể tưởng tượng, việc dựa hoàn toàn vào dữ liệu giả là rất nguy hiểm. Vì bạn đang kiểm tra mã của mình mà không giao tiếp với máy chủ thực, bạn có thể dễ dàng trở nên quá tự tin vào sức mạnh của các bài test của mình. Khi đến thời điểm sử dụng ứng dụng của bạn với dữ liệu/service thực, một thay đổi nhỏ có thể khiến mọi thứ hỏng bét. Chiến lược sau nên được sử dụng để xác nhận rằng dữ liệu bạn đang mong đợi từ máy chủ khớp với dữ liệu bạn đang kiểm tra. Mục tiêu ở đây là so sánh cấu trúc dữ liệu (ví dụ: các khóa trong một đối tượng) hơn là dữ liệu thực tế.\n",
    "\n",
    "`pytest -v`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "11. Khi thực hiện tự động hóa, bạn không cần phải chạy integration_contract 100%. Dùng biến môi trường kết hợp với skipIf để bật tắt việc chạy test này.\n",
    "\n",
    "`set SKIP_REAL=TRUE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test bao nhiêu là đủ??\n",
    "\n",
    "Độ phủ của mã là số dòng mã ước tính được thực thi bởi các testcase. Nếu chúng ta biết con số đó và tổng số dòng code trong chương trình, chúng ta có thể ước tính được số phần trăm mã đã thực sự được kiểm tra. Nếu ta có thêm một chỉ báo về những dòng nào chưa được test, ta có thể dễ dàng viết thêm các bài kiểm tra mới hơn để đảm bảo những dòng đó được cover.\n",
    "\n",
    "```\n",
    "!pip install coverage\n",
    "coverage run test_main.py       # sinh ra file .coverage\n",
    "coverage report -m test_main.py\n",
    "coverage html\n",
    "\n",
    "coverage report -m .\\test_main.py\n",
    "```\n",
    "\n",
    "Dùng pytest-cov để xem Test Coverage\n",
    "```\n",
    "pytest -v --cov=test_main --cov-report=html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test API với [reqres.in](https://reqres.in/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các tính năng khác\n",
    "* XFail\n",
    "* Tham số condition, reason, raises, run, strict\n",
    "* Chạy test song song với `pytest-xdist`\n",
    "* Tất cả trong [documentation](https://docs.pytest.org/en/latest/contents.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Mock Server\n",
    "Tự setup mock server có HTTP Response\n",
    "\n",
    "`set/export SKIP_TAGS=real`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Những công cụ khác\n",
    "* Kiểm thử trên nhiều môi trường: Dùng tox để chạy 1 bộ test trên nhiều môi trường Python với nhiều tập dependency khác nhau.\n",
    "* CI/CD - Continuous Integration/Continuous Deployment: TravisCI, Jenkin, GithubAction....: chủ yếu dùng file yaml để điều khiển quy trình tự động hóa build và test.\n",
    "* Linter:\n",
    "    * Passive Linting With flake8: `flake8 test_todos.py`\n",
    "    * Aggressive Linting With a Code Formatter\n",
    "* Testing về hiệu năng giảm sút giữa các lần thay đổi code: pytest-benchmark\n",
    "* Lỗi Security trong app với bandit"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd454e041cfbb879d13b54c8ad3696129392157108d388f38111cd59b6a48c26"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('python_tester_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
